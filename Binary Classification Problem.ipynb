{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets and Libiraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed Libiraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset (train & validation)\n",
    "train = pd.read_csv('training.csv', sep =\";\", decimal=',')\n",
    "validation = pd.read_csv('validation.csv' , sep =\";\", decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Assessing\n",
    "#### Assess datasets visually and programmatically looking for quality and tidiness issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/AXorq76Tg3Vte/giphy.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable1</th>\n",
       "      <th>variable2</th>\n",
       "      <th>variable3</th>\n",
       "      <th>variable4</th>\n",
       "      <th>variable5</th>\n",
       "      <th>variable6</th>\n",
       "      <th>variable7</th>\n",
       "      <th>variable8</th>\n",
       "      <th>variable9</th>\n",
       "      <th>variable10</th>\n",
       "      <th>variable11</th>\n",
       "      <th>variable12</th>\n",
       "      <th>variable13</th>\n",
       "      <th>variable14</th>\n",
       "      <th>variable15</th>\n",
       "      <th>variable17</th>\n",
       "      <th>variable18</th>\n",
       "      <th>variable19</th>\n",
       "      <th>classLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>1.750</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>80.0</td>\n",
       "      <td>5</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>16.92</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0.290</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>31.25</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>96.0</td>\n",
       "      <td>19</td>\n",
       "      <td>960000.0</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>48.17</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>i</td>\n",
       "      <td>o</td>\n",
       "      <td>0.335</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2320000.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variable1  variable2  variable3 variable4 variable5 variable6 variable7  \\\n",
       "0         a      17.92   0.000054         u         g         c         v   \n",
       "1         b      16.92   0.000034         y         p         k         v   \n",
       "2         b      31.25   0.000112         u         g        ff        ff   \n",
       "3         a      48.17   0.000133         u         g         i         o   \n",
       "4         b      32.33   0.000350         u         g         k         v   \n",
       "\n",
       "   variable8 variable9 variable10  variable11 variable12 variable13  \\\n",
       "0      1.750         f          t           1          t          g   \n",
       "1      0.290         f          f           0          f          s   \n",
       "2      0.000         f          t           1          f          g   \n",
       "3      0.335         f          f           0          f          g   \n",
       "4      0.500         f          f           0          t          g   \n",
       "\n",
       "   variable14  variable15  variable17 variable18  variable19 classLabel  \n",
       "0        80.0           5    800000.0          t           0        no.  \n",
       "1       200.0           0   2000000.0        NaN           0        no.  \n",
       "2        96.0          19    960000.0          t           0        no.  \n",
       "3         0.0         120         0.0        NaN           0        no.  \n",
       "4       232.0           0   2320000.0          f           0        no.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable1</th>\n",
       "      <th>variable2</th>\n",
       "      <th>variable3</th>\n",
       "      <th>variable4</th>\n",
       "      <th>variable5</th>\n",
       "      <th>variable6</th>\n",
       "      <th>variable7</th>\n",
       "      <th>variable8</th>\n",
       "      <th>variable9</th>\n",
       "      <th>variable10</th>\n",
       "      <th>variable11</th>\n",
       "      <th>variable12</th>\n",
       "      <th>variable13</th>\n",
       "      <th>variable14</th>\n",
       "      <th>variable15</th>\n",
       "      <th>variable17</th>\n",
       "      <th>variable18</th>\n",
       "      <th>variable19</th>\n",
       "      <th>classLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>bb</td>\n",
       "      <td>1.585</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>s</td>\n",
       "      <td>420.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4200000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>23.58</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>0.540</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1360000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>36.42</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>d</td>\n",
       "      <td>v</td>\n",
       "      <td>0.585</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2400000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b</td>\n",
       "      <td>18.42</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.125</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>120.0</td>\n",
       "      <td>375</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>120.0</td>\n",
       "      <td>475</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variable1  variable2  variable3 variable4 variable5 variable6 variable7  \\\n",
       "0         b      32.33   0.000750         u         g         e        bb   \n",
       "1         b      23.58   0.000179         u         g         c         v   \n",
       "2         b      36.42   0.000075         y         p         d         v   \n",
       "3         b      18.42   0.001041         y         p        aa         v   \n",
       "4         b      24.50   0.001334         y         p        aa         v   \n",
       "\n",
       "   variable8 variable9 variable10  variable11 variable12 variable13  \\\n",
       "0      1.585         t          f           0          t          s   \n",
       "1      0.540         f          f           0          t          g   \n",
       "2      0.585         f          f           0          f          g   \n",
       "3      0.125         t          f           0          f          g   \n",
       "4      0.040         f          f           0          t          g   \n",
       "\n",
       "   variable14  variable15  variable17 variable18  variable19 classLabel  \n",
       "0       420.0           0   4200000.0        NaN           1        no.  \n",
       "1       136.0           1   1360000.0        NaN           0        no.  \n",
       "2       240.0           3   2400000.0        NaN           1        no.  \n",
       "3       120.0         375   1200000.0        NaN           0        no.  \n",
       "4       120.0         475   1200000.0          f           1        no.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3700 entries, 0 to 3699\n",
      "Data columns (total 19 columns):\n",
      "variable1     3661 non-null object\n",
      "variable2     3661 non-null float64\n",
      "variable3     3700 non-null float64\n",
      "variable4     3636 non-null object\n",
      "variable5     3636 non-null object\n",
      "variable6     3634 non-null object\n",
      "variable7     3634 non-null object\n",
      "variable8     3700 non-null float64\n",
      "variable9     3700 non-null object\n",
      "variable10    3700 non-null object\n",
      "variable11    3700 non-null int64\n",
      "variable12    3700 non-null object\n",
      "variable13    3700 non-null object\n",
      "variable14    3600 non-null float64\n",
      "variable15    3700 non-null int64\n",
      "variable17    3600 non-null float64\n",
      "variable18    1555 non-null object\n",
      "variable19    3700 non-null int64\n",
      "classLabel    3700 non-null object\n",
      "dtypes: float64(5), int64(3), object(11)\n",
      "memory usage: 549.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 19 columns):\n",
      "variable1     197 non-null object\n",
      "variable2     197 non-null float64\n",
      "variable3     200 non-null float64\n",
      "variable4     198 non-null object\n",
      "variable5     198 non-null object\n",
      "variable6     197 non-null object\n",
      "variable7     197 non-null object\n",
      "variable8     200 non-null float64\n",
      "variable9     200 non-null object\n",
      "variable10    200 non-null object\n",
      "variable11    200 non-null int64\n",
      "variable12    200 non-null object\n",
      "variable13    200 non-null object\n",
      "variable14    197 non-null float64\n",
      "variable15    200 non-null int64\n",
      "variable17    197 non-null float64\n",
      "variable18    89 non-null object\n",
      "variable19    200 non-null int64\n",
      "classLabel    200 non-null object\n",
      "dtypes: float64(5), int64(3), object(11)\n",
      "memory usage: 29.8+ KB\n"
     ]
    }
   ],
   "source": [
    "validation.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many duplicated rows in both datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3210 Duplicated rows in train dataset\n",
      "0 Duplicated rows in validation dataset\n"
     ]
    }
   ],
   "source": [
    "print(train.duplicated().sum(), 'Duplicated rows in train dataset')\n",
    "print(validation.duplicated().sum(), 'Duplicated rows in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the distribution of classlable in train dataset?<br>Is train data balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of yes/no values of classlabel in train dataset :\n",
      "yes.    92.540541\n",
      "no.      7.459459\n",
      "Name: classLabel, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of yes/no values of classlabel in train dataset :')\n",
    "print(train['classLabel'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing results: <br><br>1- 'variable18' has 58% missing values in train and 55% missing values in validation dataset.<br><br>2- 'variable5' is redundant of 'variable4'; when it's 'u' in variable4, it's 'g' in variable5 and so on.<br><br>3- 'variable17' equals 'variable14' * 10000.<br><br>4- 'variable19' is redundant of 'classlabel'; when it's 0 in variable19, it's 'no.' in classlabel and so on.<br> <br>5- There are missing values in variables 1, 2, 4, 5, 6, 7, 14, 17.<br><br>6- There are 3210 duplicated rows in train dataset.<br><br>7- Variables 2, 3 and 8 are object datatype and have numerical values. <br><br>8- 92.5% out of train dataset classlabels are 'yes.' and only 7.5% are 'no.' ( Imbalanced data problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/kXBVtKjLxINji/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "#### Let's fix these issues that we've listed in data assessing phase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we will drop 'variable18' as it has more than 50% missing values in both datasets and variables 5,17 and 19 as they have no need and can cause overfitting to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing variable5, variable17, variable18 and variable19 from train and validation datasets.\n",
    "train.drop(['variable5', 'variable17', 'variable18', 'variable19'], axis=1, inplace=True)\n",
    "validation.drop(['variable5', 'variable17', 'variable18', 'variable19'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, we will drop duplicated rows in train dataset to keep only unique rows. As keeping duplicated rows may cause overfitting in our classification model. ( It will also solve the problem of imbalanced data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droping duplicated rows in train dataset.\n",
    "train.drop_duplicates(keep = 'first', inplace = True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the problem of imbalanced data is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of yes/no values of classlabel in train dataset :\n",
      "no.     56.326531\n",
      "yes.    43.673469\n",
      "Name: classLabel, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of yes/no values of classlabel in train dataset :')\n",
    "print(train['classLabel'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We still have missing values in both datasets, so we are going to handle them by imputing them with the most used value for categorical data and median for numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 Rows have null values in train dataset\n",
      "9 Rows have null values in validation dataset\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().any(axis = 1).sum(), 'Rows have null values in train dataset')\n",
    "print(validation.isnull().any(axis = 1).sum(), 'Rows have null values in validation dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train.columns\n",
    "for col in columns:\n",
    "    if(train[col].dtype == np.dtype('O')):\n",
    "        train[col].fillna(train[col].value_counts().idxmax(), inplace=True)\n",
    "        validation[col].fillna(validation[col].value_counts().idxmax(), inplace=True)\n",
    "    else:\n",
    "        train[col].fillna(train[col].median(), inplace=True)\n",
    "        validation[col].fillna(validation[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Rows have null values in train dataset\n",
      "0 Rows have null values in validation dataset\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().any(axis = 1).sum(), 'Rows have null values in train dataset')\n",
    "print(validation.isnull().any(axis = 1).sum(), 'Rows have null values in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we convert variables 2, 3 and 8 datatypes as they are in string datatypes and hold numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['variable2', 'variable3', 'variable8']] = train[['variable2', 'variable3', 'variable8']].astype(float)\n",
    "validation[['variable2', 'variable3', 'variable8']] = validation[['variable2', 'variable3', 'variable8']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we have clean datasets without missing values or duplicated rows and with the right datatype for every variable. Train dataset has 490 rows and validation dataset has 200 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 490 entries, 0 to 489\n",
      "Data columns (total 15 columns):\n",
      "variable1     490 non-null object\n",
      "variable2     490 non-null float64\n",
      "variable3     490 non-null float64\n",
      "variable4     490 non-null object\n",
      "variable6     490 non-null object\n",
      "variable7     490 non-null object\n",
      "variable8     490 non-null float64\n",
      "variable9     490 non-null object\n",
      "variable10    490 non-null object\n",
      "variable11    490 non-null int64\n",
      "variable12    490 non-null object\n",
      "variable13    490 non-null object\n",
      "variable14    490 non-null float64\n",
      "variable15    490 non-null int64\n",
      "classLabel    490 non-null object\n",
      "dtypes: float64(4), int64(2), object(9)\n",
      "memory usage: 61.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 15 columns):\n",
      "variable1     200 non-null object\n",
      "variable2     200 non-null float64\n",
      "variable3     200 non-null float64\n",
      "variable4     200 non-null object\n",
      "variable6     200 non-null object\n",
      "variable7     200 non-null object\n",
      "variable8     200 non-null float64\n",
      "variable9     200 non-null object\n",
      "variable10    200 non-null object\n",
      "variable11    200 non-null int64\n",
      "variable12    200 non-null object\n",
      "variable13    200 non-null object\n",
      "variable14    200 non-null float64\n",
      "variable15    200 non-null int64\n",
      "classLabel    200 non-null object\n",
      "dtypes: float64(4), int64(2), object(9)\n",
      "memory usage: 23.6+ KB\n"
     ]
    }
   ],
   "source": [
    "validation.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, it's time to split our datasets to dependent and independent variables (X,Y) and prepare them to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, :-1]\n",
    "Y_train = train.iloc[:, -1]\n",
    "X_valid = validation.iloc[: , :-1]\n",
    "Y_valid = validation.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding\n",
    "#### First step in incoding our data is to encode classlabels to be 0 or 1 instead of 'yes.' or 'no.' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Classlabel data to be 0 or 1.\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y_train = labelencoder_Y.fit_transform(Y_train)\n",
    "Y_valid = labelencoder_Y.fit_transform(Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables in both train and validation datasets using get_dummies() function and drop the original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = X_train.columns\n",
    "# For every column in both train and validation datasets:\n",
    "for col in columns:\n",
    "    # If the coulmn has categorical values 'Object datatype':\n",
    "    if(X_train[col].dtype == np.dtype('O')):\n",
    "        # Get the dummy varaibles of this coulmn in train and validation datasets and add the name of the column as prefix.\n",
    "        t_dummies = pd.get_dummies(X_train[col], prefix=col, prefix_sep='_')\n",
    "        v_dummies = pd.get_dummies(X_valid[col], prefix=col, prefix_sep='_')\n",
    "        # Add 'concatinate' these varaibles to the datasets\n",
    "        X_valid = pd.concat([X_valid , v_dummies] , axis = 'columns')\n",
    "        X_train = pd.concat([X_train , t_dummies] , axis = 'columns')\n",
    "        # Drop the original column in both datasets\n",
    "        X_train.drop(col, axis=1, inplace=True)\n",
    "        X_valid.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we have a problem that there are some dummy vairables exist in trian dataset and don't in validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of two datasets are not the same\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_train.columns == X_valid.columns\n",
    "except:\n",
    "    print('Lengths of two datasets are not the same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To solve this problem, we are going to add every column in train dataset that doesn't exist in validation dataset with 0 values and in the same index in validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncol = len (X_train.columns)\n",
    "for i in range (Ncol):\n",
    "    if (X_train.columns[i] in X_valid.columns) == 0:\n",
    "        X_valid.insert(i, X_train.columns[i], 0)\n",
    "        \n",
    "X_train.sort_index(axis=1, inplace=True)\n",
    "X_valid.sort_index(axis=1, inplace=True)        \n",
    "variables = X_train.columns        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns == X_valid.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling\n",
    "#### We are going now to re-scale the data from 0 to 1 to be more fast and efficient in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_valid = sc_X.transform(X_valid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling\n",
    "#### That is the last step, the data is ready now for modeling. We will try deffrent machine learning models and choose the best of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/FA77mwaxV74SA/giphy.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed( 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start with simple logistic regression model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with logistic classifier is  87.0 %\n"
     ]
    }
   ],
   "source": [
    "# Train and predict data with Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_classifier = LogisticRegression(solver = 'lbfgs' , random_state = 0)\n",
    "LR_classifier.fit(X_train, Y_train)\n",
    "y_pred  = LR_classifier.predict(X_valid)\n",
    "acc_log = accuracy_score(Y_valid , y_pred) * 100\n",
    "print(\"Accuracy with logistic classifier is \" , acc_log , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearst Neighbors model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with K-Nearest Neighbors Classifier (KNN) is  87.5 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = 16)\n",
    "knn_classifier.fit(X_train, Y_train)\n",
    "y_pred = knn_classifier.predict(X_valid)\n",
    "acc_KNN = accuracy_score(Y_valid , y_pred) * 100\n",
    "print(\"Accuracy with K-Nearest Neighbors Classifier (KNN) is \" , acc_KNN , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Support Vector Classifier (SVC) is  86.0 %\n"
     ]
    }
   ],
   "source": [
    "# Train and predict data with Support Vector Classifier (SVC)\n",
    "from sklearn.svm import SVC\n",
    "svc_classifier = SVC(kernel = 'rbf', random_state = 0, gamma= 0.66)\n",
    "svc_classifier.fit(X_train, Y_train)\n",
    "y_pred = svc_classifier.predict(X_valid)\n",
    "acc_SVC = accuracy_score(Y_valid , y_pred) * 100\n",
    "print(\"Accuracy with Support Vector Classifier (SVC) is \" , acc_SVC , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest Classifier model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with RandomForest Classifier is  87.5 %\n"
     ]
    }
   ],
   "source": [
    "# Train and predict data with RandomForest Classifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 10 , criterion = 'entropy' , random_state = 0)\n",
    "rf_classifier.fit(X_train , Y_train) \n",
    "y_pred = rf_classifier.predict(X_valid)\n",
    "acc_RF = accuracy_score(Y_valid , y_pred) * 100\n",
    "print(\"Accuracy with RandomForest Classifier is \" , acc_RF , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple neural network using MLPClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy with MLP is:  87.0 %\n",
      "Test accuracy with MLP is:  86.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_classifier = MLPClassifier(solver='lbfgs', alpha=0.01, hidden_layer_sizes=(10, 7, 5, 5), random_state=1)\n",
    "mlp_classifier.fit(X_train, Y_train)\n",
    "train_predictions = mlp_classifier.predict(X_train)\n",
    "test_predictions = mlp_classifier.predict(X_valid)\n",
    "train_acc = round(accuracy_score(Y_train, train_predictions) , 2) * 100\n",
    "test_acc = round(accuracy_score(Y_valid, test_predictions) , 2) * 100\n",
    "print(\"Train accuracy with MLP is: \", train_acc , '%')\n",
    "print(\"Test accuracy with MLP is: \", test_acc , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we tried to design neural network using keras?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\WinDows\\Anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\WinDows\\Anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "490/490 [==============================] - 0s 737us/step - loss: 0.8311 - accuracy: 0.5551\n",
      "Epoch 2/100\n",
      "490/490 [==============================] - 0s 92us/step - loss: 0.8096 - accuracy: 0.5878\n",
      "Epoch 3/100\n",
      "490/490 [==============================] - 0s 92us/step - loss: 0.7912 - accuracy: 0.5898\n",
      "Epoch 4/100\n",
      "490/490 [==============================] - ETA: 0s - loss: 0.7804 - accuracy: 0.65 - 0s 94us/step - loss: 0.7751 - accuracy: 0.6061\n",
      "Epoch 5/100\n",
      "490/490 [==============================] - 0s 96us/step - loss: 0.7602 - accuracy: 0.6694\n",
      "Epoch 6/100\n",
      "490/490 [==============================] - 0s 98us/step - loss: 0.7438 - accuracy: 0.7531\n",
      "Epoch 7/100\n",
      "490/490 [==============================] - 0s 81us/step - loss: 0.7252 - accuracy: 0.8469\n",
      "Epoch 8/100\n",
      "490/490 [==============================] - 0s 88us/step - loss: 0.7004 - accuracy: 0.8490\n",
      "Epoch 9/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.6650 - accuracy: 0.8347\n",
      "Epoch 10/100\n",
      "490/490 [==============================] - 0s 92us/step - loss: 0.6285 - accuracy: 0.8429\n",
      "Epoch 11/100\n",
      "490/490 [==============================] - 0s 90us/step - loss: 0.5989 - accuracy: 0.8469\n",
      "Epoch 12/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.5761 - accuracy: 0.8510\n",
      "Epoch 13/100\n",
      "490/490 [==============================] - 0s 96us/step - loss: 0.5570 - accuracy: 0.8551\n",
      "Epoch 14/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.5358 - accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "490/490 [==============================] - 0s 94us/step - loss: 0.5166 - accuracy: 0.8571\n",
      "Epoch 16/100\n",
      "490/490 [==============================] - 0s 92us/step - loss: 0.4991 - accuracy: 0.8592\n",
      "Epoch 17/100\n",
      "490/490 [==============================] - 0s 98us/step - loss: 0.4830 - accuracy: 0.8592\n",
      "Epoch 18/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.4659 - accuracy: 0.8633\n",
      "Epoch 19/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.4521 - accuracy: 0.8653\n",
      "Epoch 20/100\n",
      "490/490 [==============================] - 0s 94us/step - loss: 0.4407 - accuracy: 0.8633\n",
      "Epoch 21/100\n",
      "490/490 [==============================] - 0s 90us/step - loss: 0.4329 - accuracy: 0.8673\n",
      "Epoch 22/100\n",
      "490/490 [==============================] - 0s 92us/step - loss: 0.4273 - accuracy: 0.8735\n",
      "Epoch 23/100\n",
      "490/490 [==============================] - 0s 102us/step - loss: 0.4207 - accuracy: 0.8776\n",
      "Epoch 24/100\n",
      "490/490 [==============================] - 0s 102us/step - loss: 0.4175 - accuracy: 0.8735\n",
      "Epoch 25/100\n",
      "490/490 [==============================] - 0s 106us/step - loss: 0.4142 - accuracy: 0.8714\n",
      "Epoch 26/100\n",
      "490/490 [==============================] - 0s 104us/step - loss: 0.4123 - accuracy: 0.8755\n",
      "Epoch 27/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.4109 - accuracy: 0.8735\n",
      "Epoch 28/100\n",
      "490/490 [==============================] - 0s 102us/step - loss: 0.4105 - accuracy: 0.8694\n",
      "Epoch 29/100\n",
      "490/490 [==============================] - 0s 98us/step - loss: 0.4077 - accuracy: 0.8755\n",
      "Epoch 30/100\n",
      "490/490 [==============================] - 0s 94us/step - loss: 0.4079 - accuracy: 0.8735\n",
      "Epoch 31/100\n",
      "490/490 [==============================] - 0s 90us/step - loss: 0.4060 - accuracy: 0.8694\n",
      "Epoch 32/100\n",
      "490/490 [==============================] - 0s 90us/step - loss: 0.4048 - accuracy: 0.8776\n",
      "Epoch 33/100\n",
      "490/490 [==============================] - 0s 98us/step - loss: 0.4047 - accuracy: 0.8816\n",
      "Epoch 34/100\n",
      "490/490 [==============================] - 0s 94us/step - loss: 0.4048 - accuracy: 0.8735\n",
      "Epoch 35/100\n",
      "490/490 [==============================] - 0s 92us/step - loss: 0.4032 - accuracy: 0.8714\n",
      "Epoch 36/100\n",
      "490/490 [==============================] - 0s 104us/step - loss: 0.4030 - accuracy: 0.8776\n",
      "Epoch 37/100\n",
      "490/490 [==============================] - 0s 114us/step - loss: 0.4022 - accuracy: 0.8714\n",
      "Epoch 38/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.4014 - accuracy: 0.8796\n",
      "Epoch 39/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.4008 - accuracy: 0.8735\n",
      "Epoch 40/100\n",
      "490/490 [==============================] - 0s 102us/step - loss: 0.4017 - accuracy: 0.8796\n",
      "Epoch 41/100\n",
      "490/490 [==============================] - 0s 106us/step - loss: 0.3998 - accuracy: 0.8694\n",
      "Epoch 42/100\n",
      "490/490 [==============================] - 0s 100us/step - loss: 0.3997 - accuracy: 0.8816\n",
      "Epoch 43/100\n",
      "490/490 [==============================] - 0s 92us/step - loss: 0.3985 - accuracy: 0.8776\n",
      "Epoch 44/100\n",
      "490/490 [==============================] - 0s 90us/step - loss: 0.3979 - accuracy: 0.8857\n",
      "Epoch 45/100\n",
      "490/490 [==============================] - 0s 90us/step - loss: 0.3979 - accuracy: 0.8694\n",
      "Epoch 46/100\n",
      "490/490 [==============================] - 0s 85us/step - loss: 0.3972 - accuracy: 0.8837\n",
      "Epoch 47/100\n",
      "490/490 [==============================] - 0s 96us/step - loss: 0.3963 - accuracy: 0.8796\n",
      "Epoch 48/100\n",
      "490/490 [==============================] - 0s 88us/step - loss: 0.3957 - accuracy: 0.8837\n",
      "Epoch 49/100\n",
      "490/490 [==============================] - 0s 85us/step - loss: 0.3970 - accuracy: 0.8755\n",
      "Epoch 50/100\n",
      "490/490 [==============================] - 0s 90us/step - loss: 0.3967 - accuracy: 0.8714\n",
      "Epoch 51/100\n",
      "490/490 [==============================] - 0s 88us/step - loss: 0.3947 - accuracy: 0.8857\n",
      "Epoch 52/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3949 - accuracy: 0.8776\n",
      "Epoch 53/100\n",
      "490/490 [==============================] - 0s 75us/step - loss: 0.3940 - accuracy: 0.8755\n",
      "Epoch 54/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3936 - accuracy: 0.8796\n",
      "Epoch 55/100\n",
      "490/490 [==============================] - 0s 75us/step - loss: 0.3932 - accuracy: 0.8694\n",
      "Epoch 56/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3927 - accuracy: 0.8796\n",
      "Epoch 57/100\n",
      "490/490 [==============================] - 0s 81us/step - loss: 0.3914 - accuracy: 0.8755\n",
      "Epoch 58/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3899 - accuracy: 0.8796\n",
      "Epoch 59/100\n",
      "490/490 [==============================] - 0s 81us/step - loss: 0.3888 - accuracy: 0.8816\n",
      "Epoch 60/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3901 - accuracy: 0.8673\n",
      "Epoch 61/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3895 - accuracy: 0.8776\n",
      "Epoch 62/100\n",
      "490/490 [==============================] - 0s 88us/step - loss: 0.3871 - accuracy: 0.8735\n",
      "Epoch 63/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3864 - accuracy: 0.8776\n",
      "Epoch 64/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3849 - accuracy: 0.8776\n",
      "Epoch 65/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3844 - accuracy: 0.8755\n",
      "Epoch 66/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3832 - accuracy: 0.8776\n",
      "Epoch 67/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3849 - accuracy: 0.8735\n",
      "Epoch 68/100\n",
      "490/490 [==============================] - 0s 75us/step - loss: 0.3822 - accuracy: 0.8755\n",
      "Epoch 69/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3846 - accuracy: 0.8857\n",
      "Epoch 70/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3843 - accuracy: 0.8735\n",
      "Epoch 71/100\n",
      "490/490 [==============================] - 0s 75us/step - loss: 0.3812 - accuracy: 0.8796\n",
      "Epoch 72/100\n",
      "490/490 [==============================] - 0s 85us/step - loss: 0.3821 - accuracy: 0.8796\n",
      "Epoch 73/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3805 - accuracy: 0.8837\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490/490 [==============================] - 0s 83us/step - loss: 0.3804 - accuracy: 0.8816\n",
      "Epoch 75/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3798 - accuracy: 0.8796\n",
      "Epoch 76/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3809 - accuracy: 0.8796\n",
      "Epoch 77/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3813 - accuracy: 0.8816\n",
      "Epoch 78/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3819 - accuracy: 0.8735\n",
      "Epoch 79/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3793 - accuracy: 0.8796\n",
      "Epoch 80/100\n",
      "490/490 [==============================] - 0s 75us/step - loss: 0.3795 - accuracy: 0.8857\n",
      "Epoch 81/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3794 - accuracy: 0.8878\n",
      "Epoch 82/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3783 - accuracy: 0.8837\n",
      "Epoch 83/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3774 - accuracy: 0.8857\n",
      "Epoch 84/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3771 - accuracy: 0.8816\n",
      "Epoch 85/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3777 - accuracy: 0.8857\n",
      "Epoch 86/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3782 - accuracy: 0.8816\n",
      "Epoch 87/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3779 - accuracy: 0.8918\n",
      "Epoch 88/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3758 - accuracy: 0.8816\n",
      "Epoch 89/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3757 - accuracy: 0.8816\n",
      "Epoch 90/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3751 - accuracy: 0.8837\n",
      "Epoch 91/100\n",
      "490/490 [==============================] - 0s 75us/step - loss: 0.3748 - accuracy: 0.8816\n",
      "Epoch 92/100\n",
      "490/490 [==============================] - 0s 85us/step - loss: 0.3744 - accuracy: 0.8816\n",
      "Epoch 93/100\n",
      "490/490 [==============================] - 0s 75us/step - loss: 0.3760 - accuracy: 0.8837\n",
      "Epoch 94/100\n",
      "490/490 [==============================] - 0s 85us/step - loss: 0.3758 - accuracy: 0.8918\n",
      "Epoch 95/100\n",
      "490/490 [==============================] - 0s 83us/step - loss: 0.3740 - accuracy: 0.8898\n",
      "Epoch 96/100\n",
      "490/490 [==============================] - 0s 79us/step - loss: 0.3738 - accuracy: 0.8878\n",
      "Epoch 97/100\n",
      "490/490 [==============================] - 0s 81us/step - loss: 0.3728 - accuracy: 0.8939\n",
      "Epoch 98/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3745 - accuracy: 0.8837\n",
      "Epoch 99/100\n",
      "490/490 [==============================] - 0s 81us/step - loss: 0.3730 - accuracy: 0.8959\n",
      "Epoch 100/100\n",
      "490/490 [==============================] - 0s 77us/step - loss: 0.3746 - accuracy: 0.8898\n",
      "490/490 [==============================] - 0s 141us/step\n",
      "200/200 [==============================] - 0s 40us/step\n",
      "Train accuracy with Nueral Network Classifier is  89.18 %\n",
      "Test accuracy with Nueral Network Classifier is  86.5 %\n"
     ]
    }
   ],
   "source": [
    "# Train and predict data with Classification Neural Network\n",
    "from keras import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense\n",
    "\n",
    "nn_classifier = Sequential()\n",
    "nn_classifier.add(Dense(5, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), input_dim=43))\n",
    "nn_classifier.add(Dense(5, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01) ))\n",
    "nn_classifier.add(Dense(2, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "nn_classifier.add(Dense(1, activation='sigmoid'))\n",
    "nn_classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "nn_classifier.fit(X_train,Y_train, batch_size=20, epochs=100)\n",
    "\n",
    "train_acc = round(nn_classifier.evaluate(X_train, Y_train)[1] *100 , 2)\n",
    "test_acc = round(nn_classifier.evaluate(X_valid, Y_valid)[1] *100 , 2)\n",
    "\n",
    "print(\"Train accuracy with Nueral Network Classifier is \"  , train_acc, '%')\n",
    "print(\"Test accuracy with Nueral Network Classifier is \"  , test_acc, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling results:\n",
    "#### We have tried 6 diffrent models ( Logistic Regression, K-NN, SVC, RandomForest, MLP and cusrtom nueral netwok )<br><br>Accuracy varies between 86% and 88%. Most frequent accuracy among these models is 87% for validation dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microservice to serve the predictions\n",
    "#### let's design microservice in form of simple function that takes the data as an input and returns the predictions of our model as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    # Check for unneeded column that we dropped above and drop them, if found.\n",
    "    unneeded_cols = ['variable5', 'variable17', 'variable18', 'variable19','classLabel']\n",
    "    for col in unneeded_cols:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # Encode categorical data.\n",
    "    columns = df.columns        \n",
    "    for col in columns:\n",
    "        # If the coulmn has categorical values 'Object datatype':\n",
    "        if(df[col].dtype == np.dtype('O')):\n",
    "            # Get the dummy varaibles of this coulmn and add the name of the column as prefix\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, prefix_sep='_')\n",
    "            # Add 'concatinate' these varaibles to the dataset\n",
    "            df = pd.concat([df , dummies] , axis = 'columns')\n",
    "            # Drop the original column in both datasets\n",
    "            df.drop(col, axis=1, inplace=True)      \n",
    "            \n",
    "    # Add variables that don't exist in dataset by value 0        \n",
    "    Ncol = len (variables)\n",
    "    for i in range (Ncol):\n",
    "        if (variables[i] not in df.columns):\n",
    "            df.insert(len(df.columns), variables[i], 0)   \n",
    "    # Sort the coulmns to be in the form of train data.        \n",
    "    df.sort_index(axis=1, inplace=True)        \n",
    "    # Scale the dataset with the same scaler that we used in train data          \n",
    "    df = sc_X.transform(df)\n",
    "    \n",
    "    # Now let's predict from each model:\n",
    "    logisic_pred = LR_classifier.predict(df)\n",
    "    knn_pred = knn_classifier.predict(df)\n",
    "    svc_pred = svc_classifier.predict(df)\n",
    "    rf_pred = rf_classifier.predict(df)\n",
    "    mlp_pred = mlp_classifier.predict(df)\n",
    "    nn_pred = nn_classifier.predict_classes(df)[0]\n",
    "    preds = {'Logisic Regression Predictions' : logisic_pred,\n",
    "             'K-NN Predictions' : knn_pred,\n",
    "             'SVC Predictions' : svc_pred,\n",
    "             'RandomForest Predictions' : rf_pred,\n",
    "             'Nueral Network Predictions' : mlp_pred}\n",
    "    \n",
    "    predictions = pd.DataFrame(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test this microservice on sample of validation data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logisic Regression Predictions</th>\n",
       "      <th>K-NN Predictions</th>\n",
       "      <th>SVC Predictions</th>\n",
       "      <th>RandomForest Predictions</th>\n",
       "      <th>Nueral Network Predictions</th>\n",
       "      <th>Real ClassLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Logisic Regression Predictions  K-NN Predictions  SVC Predictions  \\\n",
       "0                               0                 0                0   \n",
       "1                               0                 0                0   \n",
       "2                               0                 0                0   \n",
       "3                               0                 0                0   \n",
       "4                               0                 0                0   \n",
       "5                               0                 0                0   \n",
       "6                               0                 0                0   \n",
       "7                               0                 0                0   \n",
       "8                               0                 0                0   \n",
       "9                               0                 0                0   \n",
       "\n",
       "   RandomForest Predictions  Nueral Network Predictions Real ClassLabel  \n",
       "0                         0                           0             no.  \n",
       "1                         0                           0             no.  \n",
       "2                         1                           0             no.  \n",
       "3                         0                           0             no.  \n",
       "4                         0                           0             no.  \n",
       "5                         0                           0             no.  \n",
       "6                         0                           0             no.  \n",
       "7                         0                           0             no.  \n",
       "8                         0                           0             no.  \n",
       "9                         0                           0             no.  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('validation.csv', sep =\";\", decimal=',').iloc[1:11 , :].reset_index(drop=True)\n",
    "classlabels = df['classLabel']\n",
    "predictions = predict(df)\n",
    "predictions['Real ClassLabel'] = classlabels\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
